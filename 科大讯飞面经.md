### 1、数组和链表的区别及分别使用场景

相同点：都是数据存储的方式

不同点：

（1）------内存-------数组在内存中连续，需要预先申请，链表采用动态内存分配方法，用next指针保存前后逻辑

（2）------访问-------数组在内存中顺序存储，因此可以通过下标访问，链表需要从头遍历

（3）------增删元素--------链表不需要额外的移动，只需要改变指针，数组需要将增加节点后的全部移动

使用场景：

（1）--------功能--------主要进行查找时，用数组；主要增删时，用链表

（2）--------内存要求---------当要求存储的大小变化不大时，且可以事先确定大小用数组；反之用链表

### 2、二叉树（多叉树）的创建、遍历

### 3、Python中类继承object和不继承的区别

Python3 默认继承

Python2 中，如果是经典类（不继承object），则会按照深度搜索去调用方法，如果是新式类（继承object），会按照广度优先去搜索，即继不继承，得到的结果不同

作用----------------

（1）继承 object 可以拥有的更多高级特性

（2）调用顺序不同


### 4、softmax，指数上溢问题

输入记为一个数组X=[x1/x2.....xn]，如果其中个某个xk特别大，它又在指数上，就会出现指数上溢（超过flaot32），如果x取很小的负数，e的x次方就会非常小，超过有效数字后为0，如果分母全部为0.就会出错产生下溢出

解决方法：每一个xk都－max(X)，此时最大值为0，避免上溢出，而分母上至少有一个为1，避免除0出错，故而避免下溢出

为什么损失函数是交叉熵

### 5、常见激活函数

sigmoid，softmax，relu，leaky-relu、tahn、ELU

### 6、sigmoid函数公式

1/(1+e^(-x))

### 7、常见排序算法及稳定性、复杂度排序

### 8、常见算法复杂度（指的是dp等？）

### 9、计算机的组成部分

### 10、缓存的分级

在大型的互联网应用中，如果缓存的是大量的数据，可以考虑多级缓存数据



### 11、10亿数据是否可以装入内存

https://blog.csdn.net/qq_35423154/article/details/108703914?ops_request_misc=&request_id=&biz_id=102&utm_term=100%E4%BA%BF%E6%95%B0%E6%8D%AE%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E8%A3%85%E5%85%A5%E5%86%85%E5%AD%98&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-108703914.142^v32^down_rank,185^v2^control&spm=1018.2226.3001.4187

  一般用位图来解决，每个无符号数据（如果不用位图）要用4字节（4B，32位）

### 12、Batch Normalization(BN) 的作用

https://www.nowcoder.com/discuss/751340?source_id=discuss_experience_nctrack&channel=-1 来源面经

BN层（批量归一化）：神经网络学习本质---学习数据分布，一旦训练集、测试集分布不同，那么网络泛化能力大大下降，另一方面，每一批训练数据的分布不同，那么网络每一次训练都要去适应不同的分布，会大大降低网络训练速度，因此，如果对每一个批量数据进行归一化，可以加快网络训练速度

本质：在网络的每一层输入的时候，又插入一个归一化层，然后再进入下一层，注意！！BN层是一个可学习，有参数的网络层

参数：γ、β，控制归一化的程度


### 13、Pytorch和Tensorflow的区别

    tensor是静态框架，训练前先构建好计算图，pytorch是动态的

### 14、torch.eval()指令的作用及其对检测器中哪些部分会影响

在评估模式下，batchNorm层，dropout层等用于优化训练而添加的网络层会被关闭，从而使得评估时不会发生偏移

### 15、PCA实现过程、意义

意义：对数据进行降维、将原本比较密集的彼此有关的特征，尽可能用彼此分开的较少的特征进行表示

### 16、简述K—means

##### （1）和分类问题的区别，啥是无监督

分类问题是有监督算法，聚类是机器自己分出几类

##### （2）初始点的选择，为什么对初始点选择敏感，有什么改进的点

##### （3）每次迭代过程简述

##### （4）迭代终止条件

##### （5）K值的选择

##### （6）时间复杂度、空间复杂度




### 17、CNN 的不同模型

### 18、ATtention

### 19、Python中可变、不可变的数据类型

-----不可变的数据类型进到function里面后，没有return的情况下怎么调用

可变类型：列表，字典

不可变类型：整型int，浮点型float，字符串型string，元组tuple

把这个类型用class包装成一个类，然后将这个类的对象传入

### 20、pooling的作用

### 21、RNN

循环神经网络，存在问题：长期记忆依赖

### 22、常见的attention机制，说明channel attention 和 self attention 的原理

### 23、CE loss 的公式，BCE loss公式

CE loss：交叉熵损失函数（多分类问题）

BCE loss：二分类交叉熵损失


-------------------------针对智能语音岗位

### 24、！！！！！！！！！！图像和语音在信号处理方面的不同

### 25、研究方向和模式识别有没有异同（模式识别是？）

### 26、池化的作用

在卷积网络中应用


### 27、relu 的变形的激活函数

泄漏型relu，ELU（指数线性激活函数）

### 28、设计模式是什么

### 29、如何防止过拟合

Dropout，增加样本多样性，earlystoping，BN,L1,L2

（1）Dropout（丢弃法）：训练神经网络时，随机丢弃一部分神经元（以及对应的连接边），来避免过拟合


本质是在训练过程中，随机删除神经元，从而让每一次训练时候的模型都是不相同的，且训练的参数更少了（类似集成学习的思想：扩大规模，但是每个基础模型更简单）

（2）BN层（批量归一化）：神经网络学习本质---学习数据分布，一旦训练集、测试集分布不同，那么网络泛化能力大大下降，另一方面，每一批训练数据的分布不同，那么网络每一次训练都要去适应不同的分布，会大大降低网络训练速度，因此，如果对每一个批量数据进行归一化，可以：

本质：在网络的每一层输入的时候，又插入一个归一化层，然后再进入下一层，注意！！BN层是一个可学习，有参数的网络层

（3）正则化

通过添加额外的惩罚项来限制网络的稀疏性

L1趋近于产生少量特征值（可以特征稀疏化）

L2会选择更多的特征，只不过这些特征都会接近于0


### 30、1*1卷积的作用

### 31、XGBoost和GBDT的区别

目标函数不同

XGboost：串行，一棵一棵增加决策树

（1）XGboost加入了正则化项，用到了二阶泰勒展开，GBDT只用到一阶导数信息

（2）GBDT以回归树作为基学习器

### 32、faster-RCNN的理解

### 33、聚类有哪些，有什么不同

### 34、LSTM（长短期记忆网络）的种类，说一下lstm内部结构，从DNN到RNN到LSTM的转变

##### RNN原理中的重点

RNN中，最早的那些层可能会停止学习（或学习梯度很小），停止学习后，RNN就会忘记在较长序列中看到的内容，因此RNN是短时记忆的

https://blog.csdn.net/v_JULY_v/article/details/89894058?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165758521116782388066380%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=165758521116782388066380&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-89894058-null-null.142^v32^pc_rank_34,185^v2^control&utm_term=LSTM&spm=1018.2226.3001.4187

##### lstm的三个门控是如何设计的，具体的计算，上一时刻保留的加本时刻更新的传递给输出门，lstm如何解决梯度弥散的问题

（1）原理

H是隐藏状态，表示短期信息（和RNN类似），额外引入细胞状态C记忆长期状态，三个门通过计算，利用上次的隐藏信息以及当次的输入，更新出本次的隐藏信息H以及细胞状态C

RNN存在问题：无法判断所需要的过去信息离当前信息点的距离，如果距离相距很长：长期依赖关系，实践证明RNN处理效果较差

LSTM是明确设计的，以避免这样的问题，长时间记住信息是LSTM的默认行为

LSTM是链式结构，重复模块具有不同的结构（RNN是相同结构），关键在于单元状态

-------LSTM能够将信息移除或添加到单元状态，称为门的结构的调节

门：有选择的让信息通过的方式

步骤一：忘记门-sigmoid层实现，决定丢弃哪些信息（丢弃哪些）

步骤二：决定在单元状态中存储哪些新信息，再次分为两步：输入门层-sigmoid层实现，决定更新哪些值，tanh层-产生一个候选值向量

步骤三：输出门-先sigmoid再tanh

##### lstm如何解决梯度消失/梯度爆炸的问题

通过遗忘门sigmoid函数进行处理，接近于1，说明任务比较依赖历史信息，这个时候因为接近1，梯度不容易消失，如果接近0，也侧面说明这个信息对于任务不那么重要，这个时候消失了也没关系

梯度爆炸：通过梯度裁剪，一旦超过某个梯度，就默认梯度是当前设定的阈值，而不再增加

##### LSTM 通过相加而不是相乘的方式来计算新状态


### 35、PCA和LDA的区别

PCA：基于方差提取特征，无监督

LDA（线性判别分析）：基于标签进行降维，有监督，最多降低到类别数k-1的维数

### 36、常用语音算法，声纹识别算法

### 37、Python判断变量类型，判断一个变量是否是某个类型

  a.isinstance() 判断是否是某种类型

  a.type() 判断类型

### 38、Python元组和列表的区别

  元组：可以存放不同类型的元素，一旦定义，不能更改，不能删除，安全性高，可用于描述一个不会改变的事务的多个属性

  列表：也可以存放不同类型元素，可以修改，灵活性高

### 39、梯度消失、爆炸解决方案

梯度消失（更常见）：当网络输出很大时，通过激活函数，比如sigmoid，输出后的值会很小，多层叠加后，很容易造成梯度消失。因为是反向传播，所以越靠近输入层的，越会出现梯度消失，则深度网络退化成浅层网络（因为实际之后后面几层网络在更新、学习，前面的基本不变）

解决方法：换成relu等激活函数、用 BN 层、用残差

梯度爆炸：反向传播过程中，每一次求梯度都是＞1的，则随着层数增大，求出的梯度会指数型增长

解决方法：正则化

### 40、交叉熵

事情发生的概率越小，信息量越大

机器学习中，用交叉熵来衡量两个独立概率分布的差异

（本来是用相对熵表示两个概率分布的差异，但是相对熵=交叉熵-信息熵=H（p,q）-H（p），其中，p可以看做实际的概率分布，是已知的，可以看做常数，所以可以直接用交叉熵H（p,q）来衡量差异）

多分类问题通过softmax获得分成每一种类的概率，再用交叉熵当做损失函数

----------优点------------（为什么分类问题多用这个）

（1）分类问题中，大多损失函数是非凸的，如果用梯度下降法，可能无法到达全局最优，而交叉熵是凸的

（2）交叉熵曲线整体是单调的，损失越大，梯度越大，便于加快反向传播速度


### 41、GRU

LSTM的变体：门控循环单元，简称GRU

单个门控制器控制忘记门和输入门，另一个新门不再是输出门的结构，而是新增的一个门控制器，控制之前状态的哪一步呈现给主层


### 42、python深拷贝、浅拷贝

深拷贝：自己开辟内存区域，将内容全部拷贝，改变拷贝内容不影响原始内容，但是耗时长且占用额外内存空间

浅拷贝：只复制原始数据的地址，修改会影响原内容，但是耗时短，占用空间少

### python2和python3区别

（1）print区别

（2）默认编码不同：python2是ascll码，python3是UTF-8

（3）object继承问题

### python中is和==的区别

==是比较两个对象的值

is比较的是两个对象的id（身份标识码，即地址？）
