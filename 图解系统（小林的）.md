### 内核空间、用户空间

  - 内核空间：这个内存空间只有内核程序可以访问
  
  - 用户空间：这个内存空间专门给应用程序使用
 
 
### 存储器

```
  1、CPU 中的寄存器（处理速度最快）
  
  2、CPU 缓存：L1/L2/L3 三层
  
  -------------------------以上都在CPU 内部，所以读写速度很快----------------------
  
  3、内存：如果断电会丢失
  
  4、硬盘：永久性存储，读写速度相比内存差好几个数量级
```
  我们从图书馆书架取书，把书放到桌子上，再阅读书，我们大脑就会记忆知识点，然后再经过大脑思考，这一系列过程相当于，数据从硬盘加载到内存，再从内存加载到 CPU 的寄存器和 Cache 中，然后再通过 CPU 进行处理和计算
  
# 虚拟内存

### 如何管理虚拟地址与物理地址之间的关系？

  内存分段、内存分页
  
  如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了
  
### 内存分段

  虚拟地址：段号 + 段内偏移
  
  - 分段机制会把程序的虚拟地址分成 4 个段
  
  - 好处：产生连续内存空间，解决了程序本身不需要关心具体的物理内存地址的问题
  
  - 存在问题：会产生内存碎片，内存交换（把占内存空间很大的程序写到磁盘上，再写回来）可以解决，但是磁盘I/O很慢，即内存交换效率低
    - 内部内存碎片
    - 外部内存碎片
  
  
  
### 内存分页

  分页是把整个虚拟和物理内存空间切成⼀段段固定的大小。这样⼀个连续并且尺寸固定的内存空间，我们叫页（Page）。在 Linux 下，每⼀页的大小为 4KB
  
  - 页表：存储在内存中的
  
    - 当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常
    - 然后进入系统内核空间分配物理内存、更新进程页表
    - 最后再返回用户空间，恢复进程的运行
  
  - 存在问题：内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费（之前是段间存在内存碎片，现在是页内）
  
  - 如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为换出（Swap Out）
  - 一旦需要的时候，再加载进来，称为换入（Swap In）
  - 所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，内存交换的效率就相对比较高
  
  
### 多级页表

- 简单的分页有空间上的缺陷
  - 在 32 位的环境下，虚拟地址空间共有 4GB
  - 假设一个页的大小是 4KB（2^12）
  - 那么就需要大约 100 万 （2^20） 个页
  - 每个「页表项」需要 4 个字节大小来存储
  - 那么整个 4GB 空间的映射就需要有 4MB 的内存来存储页表
  - 那么，100 个进程的话，就需要 400MB 的内存来存储页表，这是非常大的内存了

- 解决方法：多级页表（解决空间上的问题）
  - 如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间
  - 如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了
  - 即：可以在需要时才创建二级页表

- 多级页表中，地址转换也得多级（时间上出现了问题）
- 解决办法：




### 段页式内存管理

- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制

- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页

这样，地址结构就由段号、段内页号和页内位移三部分组成


### 虚拟内存作用

- 可以使得进程对运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域

- 由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题

- 页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性

### 内存分配过程

  应用程序通过 malloc 函数申请内存的时候，实际上申请的是虚拟内存，此时并不会分配物理内存
  
  当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU 就会产生缺页中断
  
  缺页中断处理函数会看是否有空闲的物理内存，如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系
  
  如果没有空闲的物理内存，那么内核就会开始进行回收内存的工作，回收的方式主要是两种：直接内存回收和后台内存回收
  
### 内存回收

  直接内存回收、后台内存回收
  
  - 直接内存回收：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是同步的，会阻塞进程的执行
  
  - 后台内存回收：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程异步的，不会阻塞进程的执行
  
  - 如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请： ——触发 OOM （Out of Memory）机制
  
    - OOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源
    - 如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置
  
### 哪些内存可以被回收

  - 文件页：大部分文件页，都可以直接释放内存，以后有需要时，再从磁盘重新读取就可以了，回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存
  
  - 匿名页：如堆、栈等，这部分内存很可能还要再次被访问，所以不能直接释放内存，它们回收的方式是通过 Linux 的 Swap 机制，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了
  
  --都是基于LRU算法
  
  底层：维护着 active（活跃内存页链表） 和 inactive （不活跃内存页链表）两个双向链表
  
  回收内存的操作基本都会发生磁盘 I/O 的，如果回收内存的操作很频繁，意味着磁盘 I/O 次数会很多，这个过程势必会影响系统的性能，整个系统给人的感觉就是很卡
  
### 解决内存回收带来的性能影响

  - 调整参数，使得每次回收尽量先回收干净的文件页
  
  - 尽早触发后台回收
  
### 如何保护一个进程不被 OOM 杀掉

  如果你想某个进程无论如何都不能被杀掉，那你可以将 oom_score_adj 配置为 -1000
  
### 先Swap，其次还不行才会OOM

将内存数据换出磁盘，又从磁盘中恢复数据到内存的过程，就是 Swap 机制负责的




# Linux 命令

### 性能指标

TCP/IP 模型由应用层、传输层、网络层和网络接口层，共四层组成

- ls -lh 文件名：命令查看日志文件的大小

- cat ：用来查看文件内容的，不适用于大文件

- less： less 并不会加载整个文件，而是按需加载，先是输出一小页的内容，当你要往下看的时候，才会继续加载

- tail -n 数字：看日志最新部分的内容

# 调度算法

### 进程调度

1、先来先服务

2、最短作业优先

3、高响应比优先：权衡了短作业和长作业

4、CPU时间片轮转

5、最高优先级调度

6、多级反馈队列调度
```
  多个队列，优先级从高到低，分配到的CPU时间片从小到大：对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也会更长了，所以该算法很好的兼顾了长短作业，同时有较好的响应时间
```

### 页面置换算法

- 先进先出置换FIFO

- 最近最久未使用LRU

- 最不常用置换算法LFU：当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰

### 磁盘调度算法

目的：提高磁盘的访问性能，一般是通过优化磁盘的访问请求顺序来做到的

原因：访问磁盘时候要寻找要访问的地方，这个过程称为"寻道"，这个过程很耗费时间

- 先来先服务

- 最短寻道时间优先：优先选择从当前磁头位置所需寻道时间最短的请求，but 可能磁头在一小块区域来回移动

- 扫描算法：磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向，这就是扫描（Scan）算法

### 最基本的 Socket 模型（基本只能一对一通信）

同步阻塞方式

给这个 Socket 绑定一个 IP 地址和端口

- 绑定端口的目的：当内核收到 TCP 报文，通过 TCP 头里面的端口号，来找到我们的应用程序，然后把数据传递给我们.

- 绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址，当绑定一个网卡时，内核在收到该网卡上的包，才会发给我们.

### 服务器单机理论最大能连接多少个客户端？

 TCP 连接是由四元组唯一确认的，这个四元组就是：本机IP, 本机端口, 对端IP, 对端端口
 
 对于服务端 TCP 连接的四元组只有对端 IP 和端口是会变化的，所以最大 TCP 连接数 = 客户端 IP 数×客户端端口数
 
# 进程、线程

### 线程的状态（5种）以及状态间的切换

  - 新建状态：新建了一个线程对象
  
  - 就绪状态：可运⾏，由于其他进程处于运⾏状态⽽暂时停⽌运⾏
  
  - 运行状态：拿到使用权，开始运行
  
  - 阻塞状态：该进程正在等待某⼀事件发⽣（如等待输⼊/输出操作的完成）而暂时停止运行，这时，即使给它CPU控制权，它也⽆法运⾏
  
  - 死亡状态
  
切换--------------------------------------------

  - 新建 ----→ 就绪：调用了start方法，进入线程池
  
  - 就绪 ----→ 运行：获取到了cpu使用权，即调度程序选择了这个进程
  
  - 运行 ----→ 就绪：调度程序选择了另一个进程去运行，那么当前这个只能运行转就绪
  
  - 运行 ----→ 阻塞：进程为等待事件而阻塞，调用了sleep/wait/join，或者i/o请求
  
  - 阻塞 ----→ 就绪：上述那些结束了（事件完成），此时等待cpu时间片
  
  - 运行 ----→ 死亡：线程执行结束或者异常退出
  
  
----------------------挂起状态-----------------

如果有大量处于阻塞状态的进程，进程可能会占用着物理内存空间，显然不是我们所希望的，毕竟物理内存空间是有限的，被阻塞状态的进程占用着物理内存就一种浪费物理内存的行为

所以，在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，等需要再次运行的时候，再从硬盘换入到物理内存，叫挂起

 - 阻塞挂起：进程在硬盘中并等待某个事件的完成，不占用物理内存
  
 - 就绪挂起：进程在硬盘中，但只要进入内存，立刻运行
  
    - 阻塞挂起 ----→ 就绪挂起：事件完成
  

### 进程的控制结构

- 在操作系统中，是用进程控制块（process control block，PCB）数据结构来描述进程的

- PCB 是进程存在的唯一标识，这意味着一个进程的存在，必然会有一个 PCB，如果进程消失了，那么 PCB 也会随之消失

通常是通过链表的方式进行组织，把具有相同状态的进程链在一起，组成各种队列，比如：
- 将所有处于就绪状态的进程链在一起，称为就绪队列
- 把所有因等待某事件而处于等待状态的进程链在一起就组成各种阻塞队列


### 多进程模型

把每一个客户端和服务端的连接，分配一个进程去处理

当「子进程」退出时，实际上内核里还会保留该进程的一些信息，也是会占用内存的，如果不做好“回收”工作，就会变成僵尸进程，随着僵尸进程越多，会慢慢耗尽我们的系统资源

- 存在问题：当客户端数量高达一万时，肯定扛不住的，因为每产生一个进程，必会占据一定的系统资源，而且进程间上下文切换的“包袱”是很重的，性能会大打折扣

--------进程是由内核管理和调度的，所以进程的切换只能发生在内核态----------

- 进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源
- 还包括了内核堆栈、寄存器等内核空间的资源



### Python创建多进程的两种方式

由于GIL的存在,python中的多线程其实并不是真正的多线程,如果想要充分地使用多核 CPU 的资源,在python中大部分需要使用进程

- 调用库：from multiprocessing import Process
  - p = Process(target=f, args=('reid',))   创建一个进程对象,再传入一个函数f为参数
  - 使用start开启进程
  
- 通过子类继承Process类
  - 子类中必须有run方法，里面实现进程功能
  - 创建实例化对象后，调用对象的start方法，会执行run中的代码
```
  class MyProcess(Process):    # 继承multiprocessing下的Process
    def __init__(self,name):
          super().__init__()
          self.name=name       # 继承父类的init方法
```
### 多线程模型（由TCB控制）

因为一个进程下的线程的上下文切换开销比多进程小

- 问题：如果每来一个连接就创建一个线程，线程运行完后，还得操作系统还得销毁线程，虽说线程切换的上写文开销不大，但是如果频繁创建和销毁线程，系统开销也是不小的

- 解决方法：线程池

- 问题2.0：上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K，意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的

同一个进程之间的线程可以共享的有（这些资源在上下文切换时是不需要修改的）：
- 共享代码段
- 数据段
- 打开的文件
- 相同的地址空间（虚拟内存共享）
- 全局变量
- 静态变量（？）

不能共享的有：
- 每个线程各自都有一套独立的寄存器和栈
- 线程之间数据传递的时候，不需要经过内核，这就使得线程之间的数据交互效率更高了

### 线程池参数、有哪些常见线程池

  - 1、corePoolSize：线程池中的常驻核心线程数
  
  - 2、maximumPoolSize：最大线程容量
  
  - 3、keepAliveTime：多余的空闲线程存活时间
  
  - 4、unit：keepAliveTime的单位
  
  - 5、workQueue：任务队列，被提交但尚未被执行的任务
  
  - 6、threadFactory：表示生成线程池中工作线程的线程工厂，用户创建新线程，一般用默认即可
  
  - 7、handler：拒绝策略，表示当线程队列满了并且工作线程大于等于线程池的最大容量(maxnumPoolSize)时如何来拒绝请求执行的runnable的策略
  
### 线程池执行、提交任务的过程

- 首先我们提交第一个任务到线程池
- 判断线程池中的常驻核心线程数corePoolSize是否都在执行任务
  - 否：启动线程之一来执行任务
  - 是：下一步
- 判断任务队列workQueue是否满了
  - 否：将任务放入队列，等待运行
  - 是：下一步
- 判断是否达到最大容量
  - 否：新启动线程（非核心线程）来执行任务
  - 是：下一步
- 启动拒绝策略


### 进程和线程的区别

  - 1、进程是操作系统资源分配的最小单位，而线程是任务的最小单位
  
    - 一个进程里可以创建多个线程，这里面的线程共享进程的资源
  
  - 2、进程拥有一个完整的操作系统分配的资源，而线程则是共享（所属进程的）资源，只独享必不可少的资源，比如寄存器和栈，保证相对独立
  
  - 3、线程能减少并发执行的时间和空间（线程创建快，终止要释放的资源也少，且一个进程里的线程切换更快，因为他们有相同的地址空间，最后，一个进程里的线程通信也快，因为他们共享内存和文件资源）


### 进程的调度算法

  非抢占式调度算法：挑选⼀个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外⼀个进程，也就是说不会理时钟中断这个事情
  
  抢占式调度算法（cpu 时间片方式）：挑选⼀个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外⼀个进程
  
  ------------------调度算法原则--------------
  
  确保cpu利用率、系统吞吐量、周转时间（运行+阻塞）、等待时间（就绪状态）、响应时间（主要标准）
  
  - 1、先来先服务 FCFS 算法：适⽤于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统
  
  - 2、最短作业优先调度算法：优先选择运行时间最短的进程来运行，这有助提高系统的吞吐量
  
  - 3、高响应比优先调度算法（HRRN）：权衡了短作业和长作业
    - 每次进行进程调度时，先计算「响应比优先级」
    - 「响应比优先级」=（等待时间+要求服务时间）/要求服务时间
      - 要求服务时间越短，优先级越高
      - 等待时间越长，优先级越高
  
  - 4、时间片轮转调度算法（RR）：⼀般来说，时间片设为 20ms~50ms
  
  - 5、最高优先级调度算法：静态优先级、动态优先级
    - 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程
    - 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行
  
  - 6、多级反馈队列调度算法（MFQ）
    - 「多级」表示有多个队列
      - 每个队列优先级从高到低
      - 同时优先级越高时间片越短
    - 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列
      - 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度
      - 如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推
      - 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行
      - 如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行



### 操作系统给进程分配的资源具体是什么 

- CPU使用时间
- 磁盘I/O（？）
- 虚拟内存空间

### 线程的共享资源、独占资源分别是什么

独占资源：
- 程序计数器：记录线程执行位置，方便下次获得cpu时候继续执行
- 一组寄存器
- 栈：是个线程独有的，保存其运行状态和局部自动变量的
- TCB的id号等

共享资源：
- 所属进程的虚拟内存（？）
- 进程的代码段、数据段
- 全局变量
- 静态变量（？）
- 堆空间：是大家共有的空间，分全局堆和局部堆
- 打开的文件

由于共享了进程的资源，同一个进程下的线程可以很方便的进行通信


### 进程之间通信的几种方式

每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核


  - 管道：实现简单，但通信方式效率低，不适合进程间频繁地交换数据

    - 如命令 ps auxf | grep mysq 

    - 上面命令行里的「 | 」竖线就是⼀个管道，它的功能是将前⼀个命令（ ps auxf ）的输出，作为后⼀个命令（ grep mysql ）的输入

    - 管道是单向通信
      - 数据写入管道后，只有当管道里的数据被读完后，命令才可以正常退出
      - 否则阻塞到这里

    - 分类：匿名管道（用完就销毁）、命名管道（也被叫做FIFO，先进先出的传输方式）

    - 匿名管道通信范围：存在父子关系的进程
      - 这两个描述符都是在一个进程里面，并没有起到进程间通信的作用，怎么样才能使得管道是跨过两个进程的呢？
      - 我们可以使用 fork 创建子进程，创建的子进程会复制父进程的文件描述符
      - 这样就做到了两个进程各有两个「 fd[0] 与 fd[1]」
      - 两个进程就可以通过各自的 fd 写入和读取同一个管道文件实现跨进程通信了

    - 命名管道：可以在任意不相关的进程间通信
    - 不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取


  - 消息队列
    - 是保存在内核中的消息链表

    - A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了
    - B 进程需要的时候再去读取数据就可以了
    - 解决了管道效率低下的问题

    - 消息队列⽣命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会⼀直存在，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁

    - 缺点：通信不及时，且通信的数据大小也有限制，不适合较大数据传输

    - PS：消息队列通信过程中，存在用户态与内核态之间的数据拷拷贝开销，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另⼀进程读取内核中的消息数据时，会发⽣从内核态拷贝数据到用户态的过程
  

  
  - 共享内存
  
    - ！！！！！！！！！用来解决管道、消息队列中，用户态和内核态的切换的开销问题
  
    - 原理：拿出⼀块虚拟地址空间来，映射到相同的物理内存中
      - 现代操作系统，对于内存管理，采用的是虚拟内存技术
      - 也就是每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存映射到不同的物理内存中
      - 所以，即使进程 A 和 进程 B 的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响
  
  - 信号量
    - ！！！！！一种保护机制！ 
  
    - 共享内存带来的问题：是如果多个进程同时修改同⼀个共享内存，很有可能就冲突了，所以需要保护机制
    
    - 信号量就是充当保护机制，实现进程间的互斥与同步
    - 互斥就初始信号量设置为1（如果同一时间只想让一个进程操作）
    - 同步就设置为0，只有当生产者进行V操作，让信号量+1，此时消费者的P操作才能成功
    
      - P 操作、V 操作：P操作会把信号量-1，如果此时信号量＜0，说明资源被占用，进程需要阻塞等待，否则就可以正常执行；V会+1，如果相加后小于等于0，说明当前有阻塞中的进程，会将该进程唤醒运行，否则表明没有进程在阻塞

      - P 操作用于进入共享内存前，V是离开共享后，两个操作成对出现

      - 信号量初始化为期望同时有几个进程在访问共享内存
    
  - 信号
  
    - 对于异常情况下的⼯作模式！！！！就需要用「信号」的方式来通知进程，比如 kill-9
    - 
    - 信号是进程间通信机制中唯一的异步通信机制
    
  - 套接字Socket
  
    - 用于想跨网络/不同主机上的进程之间的通信的时候

### 多线程冲突了怎么办？

- 对于共享资源，如果没有上锁，在多线程的环境里，那么就可能会发生翻车现场

- 操作系统也为每个进程创建巨大、私有的虚拟内存的假象
  - 这种地址空间的抽象让每个程序好像拥有自己的内存
  - 而实际上操作系统在背后秘密地让多个地址空间「复用」物理内存或者磁盘
  - 注意：复用！！！


线程之间是可以共享进程的资源，比如代码段、堆空间、数据段、打开的文件等资源，但每个线程都有自己独立的栈空间

- 线程不安全

  - 当多线程相互竞争操作共享变量时
  - 由于运气不好，即在执行过程中发生了上下文切换，我们得到了错误的结果
  - 即：每次运行都可能得到不同的结果，因此输出的结果存在不确定性

- 临界区：由于多线程执行操作共享变量的这段代码可能会导致竞争状态，因此我们将此段代码称为临界区
  - 它是访问共享资源的代码片段，一定不能给多线程同时执行
  - 我们希望这段代码是互斥的
  - 也就说保证一个线程在临界区执行时，其他线程应该被阻止进入临界区
  
- 互斥：解决了并发进程/线程对临界区的使用问题
- 同步：并发进程/线程在一些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步

实现互斥/同步的方法：

- 加锁：实现互斥

  - 自旋锁：当获取不到锁时，线程就会一直 while 循环，不做任何事情
    - 这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用
  - 无等待锁（互斥锁？）
    - 获取不到锁的时候，不用自旋
    - 没获取到锁的时候，就把当前线程放入到锁的等待队列，然后执行调度程序，把 CPU 让给其他线程执行
- 信号量：P/V操作，实现互斥、同步


### 死锁

- 互斥条件：指多个线程不能同时使用同一个资源
- 持有并等待：某个线程自己手里有点东西，他不会放出去，但他还需要其他的一些东西，如果得不到，就等着
- 不可抢占性：不能强行从别的线程那拿东西
- 环路等待性：指的是，在死锁发生的时候，两个线程获取资源的顺序构成了环形链
  - 如果不构成，他俩抢的也不是一个资源，当然不会死锁

解决方法：破坏任意一个条件

  - 最常用的方法就是使用资源有序分配法来破坏环路等待条件
  - 比如规定，所有线程拿资源必须先拿A，再拿B，就防止了一个人拿了A，一个人拿了B，都在等对方放开资源


### 锁

- 互斥锁：加锁失败后，线程会释放 CPU ，给其他线程（有内核态参与！！有线程上下文切换！！）

  - 当加锁失败时，内核会将线程置为「睡眠」状态
  - 等到锁被释放后，内核会在合适的时机唤醒这个线程
  - 当这个线程成功获取到锁后，继续执行
  
- 自旋锁：加锁失败后，线程会忙等待，直到它拿到锁（一直在用户态！！不会主动产生上下文切换！！）
  - 被锁住的临界区代码运行时间较短的话，用自旋锁，否则互斥锁
  - 因为互斥锁有内核态到用户态这个转变，耗时一点
  - 如果很短的临界区都用互斥锁，加锁、解锁时间比真正运行还长

- 读写锁：读锁、写锁
  - 适用于能明确区分读操作和写操作的场景
  - 当「写锁」没有被线程持有时，多个线程能够并发地持有读锁
  - 一旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，其他写线程的获取写锁的操作也会被阻塞
  - so
    - 写锁是独占锁，自旋、互斥也是独占锁
    - 读锁是共享锁， 可以被多个线程同时持有

前面仨都是悲观锁！！！！！

- 悲观锁：认为多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁

- 乐观锁：假定冲突的概率很低（全程并没有加锁！！）
  - 先修改完共享资源，再验证这段时间内有没有发生冲突
  - 如果没有其他线程在修改资源，那么操作完成
  - 如果发现有其他线程已经修改过这个资源，就放弃本次操作
  - 比如：在线文档


### 虚拟内存

在 Linux 操作系统中，虚拟地址空间的内部
- 内核空间
  - 32位：1G
  - 64位：128T
- 用户空间
  - 32位：3G
  - 64位：128T



### 线程崩溃，进程一定会崩溃吗

- 一般来说如果线程是因为非法访问内存引起的崩溃，那么进程肯定会崩溃
- 为什么系统要让进程崩溃呢
  - 主要是因为在进程中，各个线程的地址空间是共享的
  - 既然是共享，那么某个线程对地址的非法访问就会导致内存的不确定性
  - 进而可能会影响到其他线程
  - 于是操作系统干脆让整个进程崩溃（用kill 信号）



### I/O 多路复用

- 问题：为每个请求分配一个进程/线程的方式不合适

- 解决方法：能只使用一个进程来维护多个 Socket —— I/O 多路复用技术

- Select/poll/epoll 这是三个多路复用接口


### select/poll

使用线性结构存储进程关心的Socket集合

### epoll

使用红黑树

# 一致性哈希

- 大多数网站背后肯定不是只有一台服务器提供服务，因为单机的并发量和数据量都是有限的，所以都会用多台服务器构成集群来对外提供服务

- 那么多个节点（后面统称服务器为节点，因为少一个字），要如何分配客户端的请求呢？

- 其实这个问题就是「负载均衡问题」

  - 最基础的负载均衡算法：轮询，就是每个服务器给一个，下一次给下一个服务器，但是不能应对「分布式系统（数据分片的系统）」，因为分布式系统中，每个节点存储的数据是不同的

  - 哈希算法：有一个很致命的问题，如果节点数量发生了变化，也就是在对系统做扩容或者缩容时，必须迁移改变了映射关系的数据，否则会出现查询不到数据的问题

-最坏情况下所有数据都需要迁移，所以它的数据迁移规模是 O(M)，这样数据的迁移成本太高了

- 一致性哈希：避免分布式系统在扩容或者缩容时，发生过多的数据迁移

### 一致性哈希

一致哈希算法是对 2^32 进行取模运算，是一个固定的值

一致性哈希要进行两步哈希：

 - 对存储节点进行哈希计算，也就是对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希；
 - 当对数据进行存储或访问时，对数据进行哈希映射


所以，一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上

映射的结果值往顺时针的方向的找到第一个节点，就是存储该数据的节点

- 存在问题：一致性哈希算法并不保证节点能够在哈希环上分布均匀，这样就会带来一个问题，会有大量的请求集中在一个节点上

### 虚拟节点

具体做法是，不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系

好处：提高负载均衡性、提高系统稳定性（当节点变化时，会有不同的节点共同分担系统的变化，因此稳定性更高）
