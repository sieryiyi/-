# 基础

https://www.nowcoder.com/discuss/938632?type=post&order=recall&pos=&page=1&ncTraceId=&channel=-1&source_id=search_post_nctrack&gio_id=83254CC42C0A7045552E7218C6A70BEA-1663038454978

↑后端知识点合集


### 为什么要有 TCP/IP 网络模型？

- 对于同一台设备上的进程间通信，有很多种方式，比如有管道、消息队列、共享内存、信号等方式
- 而对于不同设备上的进程间通信，就需要网络通信
- 而设备是多样性的，所以要兼容多种多样的设备，就协商出了一套通用的网络协议
- 这个网络协议是分层的，每一层都有各自的作用和职责

### TCP/UDP一些细节

- UDP 也可以实现可靠传输，把 TCP 的特性在应用层上实现就可以，不过要实现一个商用的可靠 UDP 传输协议，也不是一件简单的事情

- 应用需要传输的数据可能会非常大，因此当传输层的数据包大小超过 MSS（TCP 最大报文段长度），就要将数据包分块，这样即使中途有一个分块丢失或损坏了，只需要重新发送这一个分块，而不用重新发送整个数据包

- 端口：当设备作为接收方时，传输层则要负责把数据包传给应用，但是一台设备上可能会有很多应用在接收或者传输数据，因此需要用一个编号将应用区分开来，这个编号就是端口

### 网络层

如果 IP 报文大小超过 MTU（以太网中一般为 1500 字节）就会再次进行分片

网络号+主机号

### 网络接口层

以太网就是一种在「局域网」内，把附近的设备连接起来，使它们之间可以进行通讯的技术

### 键入网址到网页显示，期间发生了什么

- 解析 URL，生产 HTTP 请求信息

- 域名解析协议：在发送 HTTP 请求信息之前，还有一项工作需要完成，那就是查询服务器域名对应的 IP 地址，因为委托操作系统发送消息时，必须提供通信对象的 IP 地址

- 从应用层传输到传输层，由TCP或者UDP将数据打包成TCP报文的形式（三次握手），发送到网络层
  - TCP报文头部有一些状态位：例如 SYN 是发起一个连接，ACK 是回复，RST 是重新连接，FIN 是结束连接等
  - TCP 要做流量控制，通信双方各声明一个窗口（缓存大小），标识自己当前能够的处理能力
  - TCP 协议里面会有两个端口，一个是浏览器监听的端口（通常是随机生成的），一个是 Web 服务器监听的端口（HTTP 默认端口号是 80， HTTPS 默认端口号是 443）
  - Web服务器是被动的，只有终端客户发起请求时才会响应

- 网络层将接收到的报文打包成IP数据包的形式

- 根据路由表规则，判断下一个转发目的地

- 两点传输 —— MAC，用ARP协议

  - 到了数据链路层，网络包还需要在 IP 头部的前面加上 MAC 头部



- 目前有了 MAC头部+IP头部+TCP头部+数据+帧尾部
  - 首部和尾部的一个重要作用是帧定界，也携带了一些必要的控制信息
  - 网络包只是存放在内存中的一串二进制数字信息，没有办法直接发送给对方
  - 因此，我们需要将数字信息转换为电信号，才能在网线上传输，也就是说，这才是真正的数据发送过程

- 发送出去的网络包会通过交换机到达下一个路由器


  - 交换机工作在 MAC 层，也称为二层网络设备，交换机的端口不具有 MAC 地址，会收下所有发送过来的包

  - 路由器是基于 IP 设计的，俗称三层网络设备，路由器的各个端口都具有 MAC 地址和 IP 地址，路由器的端口都具有 MAC 地址，只接收与自身地址匹配的包，遇到不匹配的包则直接丢弃

  - 完成包接收操作之后，路由器就会去掉包开头的 MAC 头部，重新把下一个目的地的MAC头部加上去


- 数据包抵达服务器后，服务器会先扒开数据包的 MAC 头部，查看是否和服务器自己的 MAC 地址符合，符合就将包收起来，然后一层一层上去

- 拆开TCP 的头，里面有序列号，需要看一看这个序列包是不是我想要的，如果是就放入缓存中然后返回一个 ACK，如果不是就丢弃

- TCP头部里面还有端口号， HTTP 的服务器正在监听这个端口号
 
- 于是，服务器自然就知道是 HTTP 进程想要这个包，于是就将包发给 HTTP 进程

- 服务器的 HTTP 进程看到，原来这个请求是要访问一个页面，于是就把这个网页封装在 HTTP 响应报文里

- 原样传回去，然后四次挥手断开

### 各层协议（TCP/IP网络协议，四层）

- 应用层，： HTTP、DNS、FTP 等;
- 传输层：TCP/UDP
- 网络层：IP、ICMP
- 网络接口层：ARP

### Linux收发网络包

    进程通过 Socket 接口发送数据包，数据包会被网络协议栈从上到下进行逐层处理后，才会被送到网卡队列中，随后由网卡将网络包发送出去
    
    而在接收网络包时，同样也要先经过网络协议栈从下到上的逐层处理，最后才会被送到应用程序

- 网卡是计算机里的一个硬件，专门负责接收和发送网络包
- 当网卡接收到一个网络包后，会通过 DMA 技术，将网络包写入到指定的内存地址，也就是写入到 Ring Buffer ，这个是一个环形缓冲区，接着就会告诉操作系统这个网络包已经到达

网卡告知操作系统网络包到达的方法

- 最简单的一种方式就是触发中断，也就是每当网卡收到一个网络包，就触发一个中断告诉操作系统
- but，网络包很多，一直中断，CPU就不用干别的了
- 解决方法：NAPI 机制，混合「中断和轮询」（为了解决频繁中断带来的性能开销）
  - 不采用中断的方式读取数据
  - 而是首先采用中断唤醒数据接收的服务程序，然后 poll 的方法来轮询数据
    - 当有网络包到达时，会通过 DMA 技术，将网络包写入到指定的内存地址
    - 接着网卡向 CPU 发起硬件中断，当 CPU 收到硬件中断请求后，根据中断表，调用已经注册的中断处理函数（需要先「暂时屏蔽中断」，表示已经知道内存中有数据了，告诉网卡下次再收到数据包直接写内存就可以了，不要再通知 CPU 了，这样可以提高效率，避免 CPU 不停的被中断。接着，发起「软中断」，然后恢复刚才屏蔽的中断）
  - 硬件中断处理函数做的事情很少，主要耗时的工作都交给软中断处理函数了


# HTTP 篇

超文本：文字、图片、音频、视频等的混合

### 常见状态码

- 1xx：提示信息，表示还需要后续操作

- 2xx：成功
  - 204 No Content：常见的成功状态码

- 3xx：重定向，资源位置发生变动，需要客户端重新发起请求

- 4xx：客户端错误
  - 400：表示客户端请求的报文有错误，但只是个笼统的错误
  - 403：表示服务器禁止访问资源，并不是客户端的请求出错
  - 404：表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端

- 5xx：服务器错误
  - 500：与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道
  - 503：表示服务器当前很忙，暂时无法响应客户端


### HTTP 中 GET 和 POST 请求的区别

联系：是http的两种请求方式，底层都是 TCP/IP 协议，本质上都是TCP/IP连接，但由于http的标准规定和浏览器/服务器的限制，导致在应用过程中体现出一些不同

区别：
- get请求支持后退、刷新，post请求在进行后退的时候，会重新发送请求
- get一般是获取数据，一般用于搜索排序、筛选操作，post一般是提交数据，用于修改和写入数据
- get请求的数据一般会被浏览器主动缓存，post请求不会，所以如果get请求两次都请求相同数据，则第二次的消耗时间会较少

- get请求一般是放在 URL 中的，因此安全性较差，长度也受限（是因为URL的长度被浏览器和服务器限制，HTTP协议本身对 URL长度并没有做任何规定），而post请求安全性较好，长度不受限
  - 但是并不能说 GET 不如 POST 安全的，因为 HTTP 传输的内容都是明文的，虽然在浏览器地址拦看不到 POST 提交的 body 数据，但是只要抓个包就都能看到了
  - get请求只能进行url编码（URL 规定只能支持 ASCII，所以 GET 请求的参数只允许 ASCII 字符 ）
  - post请求支持多种编码


- 通常get产生一个TCP数据包，post产生两个数据包（get的效率通常要高一些，因为用get方式请求的时候，浏览器会把http的header和body一起发送出去，成功则返回200，post先发送header，成功则返回100 continue，再发送body，成功返回200）
  - post请求的过程：
    - （1）浏览器请求tcp连接（第一次握手）
    - （2）服务器答应进行tcp连接（第二次握手）
    - （3）浏览器确认，并发送post请求头（第三次握手，这个报文比较小，所以http会在此时进行第一次数据发送）
    - （4）服务器返回100 Continue响应
    - （5）浏览器发送数据
    - （6）服务器返回200 OK响应
  - get请求的过程：
    - （1）浏览器请求tcp连接（第一次握手）
    - （2）服务器答应进行tcp连接（第二次握手）
    - （3）浏览器确认，并发送get请求头和数据（第三次握手，这个报文比较小，所以http会在此时进行第一次数据发送）
    - （4）服务器返回200 OK响应
  - 目测get的总耗是post的2/3左右，这个口说无凭，网上已经有网友进行过测试
  

    
在 HTTP 协议⾥，所谓的「安全」是指请求⽅法不会「破坏」服务器上的资源。

所谓的「幂等」，意思是多次执⾏相同的操作，结果都是「相同」的。

- 那么很明显 GET ⽅法就是安全且幂等的，因为它是「只读」操作，⽆论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。

- POST 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据就会创建多
个资源，所以不是幂等的。

### HTTP（1.1） 的特点有哪些？

特点：无状态、明文传输

- 无状态
  - 好处：服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息
  - 坏处：服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦
  - 解决方法：Cookie 技术
- 明文传输
  - 好处：在传输过程中的信息，是可方便阅读的
  - 坏处：不安全
  - 解决方法：HTTPS（数据加密）、摘要算法（防止篡改，篡改了就不能正常显示）、CA证书（验证身份）



### https具体解决方式

1、混合加密（解决窃听）：对称加密（不安全） + 非对称加密（速度慢），如果全部过程都用非对称加密，会很慢

- 在通信建立前，采用非对称加密的方式交换「会话秘钥」，后续就不再使用非对称加密
- 在通信过程中，全部使用对称加密的「会话秘钥」的方式加密明文数据

- 非对称加密：共有两个密钥，这两个密钥可以双向加解密的，比如可以用公钥加密内容，然后用私钥解密，也可以用私钥加密内容，公钥解密内容
  - 公钥加密，私钥解密：这个目的是为了保证内容传输的安全
  - 私钥加密，公钥解密：这个目的是为了保证消息不会被冒充，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的
    
    
    
2、摘要算法：在计算机里会用摘要算法（哈希函数）来计算出内容的哈希值，也就是内容的「指纹」，这个哈希值是唯一的，且无法通过哈希值推导出内容
    
3、数字证书

我们常说的数字签名算法，就是用的是[私钥加密，公钥解密]这种方式，不过私钥加密内容不是内容本身，而是对内容的哈希值加密

- 私钥是由服务端保管，然后服务端会向客户端颁发对应的公钥。如果客户端收到的信息，能被公钥解密，就说明该消息是由服务器发送的
  
  - 可以通过哈希算法来保证消息的完整性（摘要算法）
  - 可以通过数字签名来保证消息的来源可靠性（能确认消息是由持有私钥的一方发送的）
  - 还缺少身份验证的环节

完整步骤：
- 服务器将自己的公钥注册到CA机构
- CA机构用自己的（注意！）私钥，将服务器的公钥进行数字签名（即：加密），并颁发数字证书（包含数字签名、公钥、服务器信息等）
- 客户端拿到服务器的数字证书后，用CA的公钥（内置在浏览器或者操作系统里了）进行解密，如果解得开，说明这个证书确实是CA机构颁发的，证明里面包含的服务器公钥的可信
- 即：验证了服务器的身份，还拿到了它的公钥
- 然后拿着公钥，对数据进行加密，然后发给服务器
- 服务器拿自己的私钥解密
  
  
### SSL/TLS的工作流程

- 考虑到性能的问题，所以双方在加密应用信息时使用的是对称加密密钥
- 而对称加密密钥是不能被泄漏的，为了保证对称加密密钥的安全性，所以使用非对称加密的方式来保护对称加密密钥的协商
- 这个工作就是密钥交换算法负责的
- 最简单的 RSA 密钥交换算法
  - TLS 第一次握手（明文）：客户端生成一个随机数A，发给服务端
  - TLS 第二次握手（明文）：服务端生成一个随机数B，把B和自己的CA证书一起发回去
  - TLS 第三次握手（非对称加密）：客户端生成一个随机数C，用服务器的公钥加密后，发给服务器，服务端收到后，用 RSA 私钥解密，得到客户端发来的随机数
    -  于是，双方根据已经得到的三个随机数，生成会话密钥（Master Secret），它是对称密钥
    -  然后客户端发一个「Change Cipher Spec」，告诉服务端开始使用加密方式发送消息
    -  然后，客户端再发一个「Encrypted Handshake Message（Finishd）」消息，把之前所有发送的数据做个摘要
    -  再用会话密钥（master secret）加密一下，让服务器做个验证，验证加密通信是否可用和之前握手信息（明文）是否有被中途篡改过
  - TLS 第四次握手：服务器也是同样的操作，发「Change Cipher Spec」和「Encrypted Handshake Message」消息，如果双方都验证加密和解密没问题，那么握手正式完成

  
### 密钥交换算法

HTTPS 常用的密钥交换算法有两种，分别是 RSA 和 ECDHE 算法

- RSA：不支持前向保密，即如果私钥泄漏，那完了
- ECDHE：
  
  
  
  
### http 1.0 和 http 1.1 和 http 2.0

- http 1.0：
  - 每发起一个请求，都要新建一次 TCP 连接
  - 而且是串行请求
  - 报文头信息是ascll码，数据体可以是文本或者二进制
- http 1.1：
  - 长连接：只要任意一端没有明确提出断开连接，则保持 TCP 连接状态
  - 管道网络传输：
    - 即：可在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去（可以减少整体的响应时间）
    - 但是：服务器必须按照接收请求的顺序发送对这些管道化请求的响应
    - 队头堵塞：如果服务端在处理 A 请求时耗时比较长，那么后续的请求的处理都会被阻塞住
   - 所以，HTTP/1.1 管道解决了请求的队头阻塞，但是没有解决响应的队头阻塞
   - 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 Body 的部分
- http 2.0
  - 头部压缩
    - 这就是所谓的 HPACK 算法
    - 如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你消除重复的部分
    - 在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了
  - 全面采用了二进制格式：收到报文后，无需再将明文的报文转成二进制，而是直接解析二进制报文，这增加了数据传输的效率
  - 可以并行交错地发送请求和响应（用 Stream ID 来区分）
    - 客户端收到后，会根据相同的 Stream ID 有序组装成 HTTP 消息
  - 服务端不再是被动地响应，可以主动向客户端发送消息（推送）
    - 客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号


### HTTP/3 做了哪些优化？

- HTTP/1.1 中的管道（ pipeline）虽然解决了请求的队头阻塞，但是没有解决响应的队头阻塞
- HTTP/2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞 ，但是一旦发生丢包，就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞
- HTTP/2 队头阻塞的问题是因为 TCP，所以 HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP！
- UDP 发送是不管顺序，也不管丢包的，所以不会出现像 HTTP/2 队头阻塞的问题。大家都知道 UDP 是不可靠传输的，但基于 UDP 的 QUIC 协议 可以实现类似 TCP 的可靠性传输
- QUIC：伪 TCP + TLS + HTTP/2 的多路复用的协议
  - 无队头阻塞
  - 更快的建立连接
  - 连接迁移


### HTTP 缓存技术

对于一些具有重复性的 HTTP 请求，比如每次请求得到的数据都一样的，我们可以把这对「请求-响应」的数据都缓存在本地，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应了，这样的话 HTTP/1.1 的性能肯定肉眼可见的提升

- 强制缓存：只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边
- 协商缓存：某些请求的响应码是 304，这个是告诉浏览器可以使用本地缓存的资源，通常这种通过服务端告知客户端是否可以使用缓存的方式被称为协商缓存

### HTTP/1.1如何优化？

- 缓存技术（避免发送重复的http请求）
- 减少 HTTP 请求次数
  - 减少重定向请求次数：
    - 服务器上的一个资源可能由于迁移、维护等原因从 url1 移至 url2 后，而客户端不知情，它还是继续请求 url1
    - 这时服务器不能粗暴地返回错误，而是通过 302 响应码和 Location 头部，告诉客户端该资源已经迁移至 url2 了，于是客户端需要再发送 url2 请求以获得服务器的资源
    - 使用代理服务器：服务端这一方往往不只有一台服务器，比如源服务器上一级是代理服务器，然后代理服务器才与客户端通信，如果重定向的工作交由代理服务器完成，就能减少 HTTP 请求次数了
  - 合并请求：
    - 如果把多个访问小文件的请求合并成一个大的请求，虽然传输的总资源还是一样，但是减少请求，也就意味着减少了重复发送的 HTTP 头部
    - 另外由于 HTTP/1.1 是请求响应模型，如果第一个发送的请求，未收到对应的响应，那么后续的请求就不会发送
    - 于是为了防止单个请求的阻塞，所以一般浏览器会同时发起 5-6 个请求，每一个请求都是不同的 TCP 连接，那么如果合并了请求，也就会减少 TCP 连接的数量，因而省去了 TCP 握手和慢启动过程耗费的时间
  - 延迟发送请求，按需访问资源：只访问当前用户看得到/用得到的资源，当客户往下滑动，再访问接下来的资源，以此达到延迟请求，也就减少了同一时间的 HTTP 请求次数

- 通过压缩响应中的数据体，降低传输数据的大小，从而提高传输效率


### HTTPS 如何优化？

产生消耗的原因：
- SSL额外的四次握手
- 握手后的对称加密报文传输（损耗较小）

会话复用：TLS 握手的目的就是为了协商出会话密钥，也就是对称加密密钥，那我们如果我们把首次 TLS 握手协商的对称加密密钥缓存起来，待下次需要建立 HTTPS 连接时，直接「复用」这个密钥，不就减少 TLS 握手的性能损耗了吗

- 服务器必须保持每一个客户端的会话密钥，随着客户端的增多，服务器的内存压力也会越大
- 现在网站服务一般是由多台服务器通过负载均衡提供服务的，客户端再次连接不一定会命中上次访问过的服务器，于是还要走完整的 TLS 握手过程

改进方式：服务器不再缓存每个客户端的会话密钥，而是把缓存的工作交给了客户端，类似于 HTTP 的 Cookie


### 粘包问题

粘包：可能A报文的数据长度，一个数据包发不出去，那么剩余部分可能在下一个包，和B的一部分信息一起发送了，而接收端收到的时候，也不知道哪块是A的消息，其中，A和B都有的那个包发生了粘包，单独不完整的A的包发生了拆包

TCP是基于字节流的

纯裸TCP收发的这些 01 串之间是没有任何边界的，你根本不知道到哪个地方才算一条完整消息

- 产生原因：
  - 要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包
  - 待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包
  - 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包
  - 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包（？）

- 解决方式：
  - 把每条要发送的数据都包装一下，比如加入消息头，消息头里写清楚一个完整的包长度是多少，根据这个长度可以继续接收数据，截取出来后它们就是我们真正要传输的消息体
  - 发送端将每个数据包封装为固定长度（不够的可以通过补0填充）
  - 可以在数据包之间设置边界，如添加特殊符号，这样，接收端通过这个边界就可以将不同的数据包拆分开

### RPC协议（底层大部分是TCP，硬要UDP也行）

RPC（Remote Procedure Call），又叫做远程过程调用。它本身并不是一个具体的协议，而是一种调用方式



# TCP

建立一个 TCP 连接是需要客户端与服务器端达成上述三个信息的共识：
- Socket：由 IP 地址和端口号组成
- 序列号：用来解决乱序问题等
- 窗口大小：用来做流量控制

### TCP和UDP区别

- TCP是面向连接的（一对一）、UDP不面向，因此可以支持一对一、一对多、多对多
- TCP可靠，UDP不可靠
- TCP存在流量控制、拥塞控制，以保证数据传输的安全性，UDP没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率
- TCP首部比较长，有20字节（不额外使用“选项”字段时），UDP则是8字节
- TCP是字节流的，即没有边界，而UDP是一个包一个包发送的，是有边界的
- 分片：
  - TCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片
  - UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层

### 面向场景

- UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于：
  - 包总量较少的通信，如 DNS 、SNMP 等
  - 视频、音频等多媒体通信
  - 广播通信
- TCP：
  - FTP 文件传输
  - HTTP / HTTPS



### 三次握手

  只有三次握手之后才能够保证两台服务器都完全没有问题，各自具备发报和收报能力，防止出现请求超时导致脏连接

- 解释
  - TTL 网络报文的生存时间往往都会超 TCP 请求超时时间，如果两次握手就可以创建连接 ，传输数据并释放连接后，第一个超时的连接请求才到达 B 机器的话，B 机器会以为是 A 创建新连接的请求，然后确认同意创建连接
    - 因为 A 机器的状态不是 SYl_SENT ，所以直接丢弃了 B 的确认数据 ，以致最后只是 B 机器单方面创建连接完毕
  - 另外一种情况，客户端发了第一个请求连接，然后因为阻塞一直没到，然后超时后客户端发了第二个连接，然后服务端那边，首先收到了第一个连接，建立好了，返回确认号，而客户端发现确认号不是自己发的第二个连接的序列+1，则发起异常终止连接命令，这个时候第二个连接请求到达服务端，然后两房正常连接
      - 如果是两次握手，服务端在收到第一个请求连接的时候，在这边就建立了连接，可以传输数据了，而这个确认信息传到客户端，客户端发现不对，会传一个终止当前连接的信息RST，直到收到这个前，服务端一直错误的建立着历史连接（很浪费资源，建立连接也是要耗费内存等的）

- 注意！！第三次握手是可以携带数据的，前两次握手是不可以携带数据的

- Linux查看：netstat -napt

### 为什么每次建立 TCP 连接时，初始化的序列号都要求不一样呢？

（超时重传建立的连接，序列号是一样的，本题说的是建立的不同连接）

为了大概率防止历史报文被下一个相同四元组的连接接收（主要方面）

如果能正常四次挥手，由于 TIME_WAIT 状态会持续 2 MSL 时长，历史报文会在下一个连接之前就会自然消失。但是来了，我们并不能保证每次连接都能通过四次挥手来正常关闭连接。

如果一样：
- 客户端和服务端建立连接后，开始传数据，传了一个A，阻塞了没到，然后超时重传
- 然后服务端断电挂了，连接也没了
- 连接没了的情况下，收到数据包，就会返回RST报文，表示错误
- 然后客户端重新建立连接（相同四元组）
- 之前阻塞的A到了，刚好该数据包的序列号正好是在服务端的接收窗口内，所以该数据包会被服务端正常接收，就会造成数据错乱

### 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？

如果没有MSS，存在问题：当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传

原因：
- 因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传
- 当接收方发现 TCP 报文（头部 + 数据）的某一片丢失后，则不会响应 ACK 给对方
- 那么发送方的 TCP 在超时后，就会重发「整个 TCP 报文（头部 + 数据）」
- 因此，可以得知由 IP 层进行分片传输，是非常没有效率的
- 所以，为了达到最佳的传输效能 TCP 协议在建立连接的时候通常要协商双方的 MSS 值，当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了

### 三次握手时，如果分别丢失，会发生什么

- 第一次握手丢失：客户端会重传若干次，每次时间增加，如第一次1s，第二次2s，第三次4s......，当到达最大重传次数，客户端断开连接
- 第二次握手丢失：SYN-ACK 报文
  - ACK，是对第一次握手的确认报文
  - SYN，是服务端发起建立 TCP 连接的报文
  - 如果客户端迟迟没有收到第二次握手，那么客户端就觉得可能自己的 SYN 报文（第一次握手）丢失了，于是客户端就会触发超时重传机制，重传 SYN 报文
  - 如果第二次握手丢失了，服务端就收不到第三次握手，于是服务端这边会触发超时重传机制，重传 SYN-ACK 报文
  - 即：两边都会触发重传
- 第三次握手丢失：ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文，即服务端重传 SYN-ACK 报文

### SYN 攻击

- 假设攻击者短时间伪造不同 IP 地址的 SYN 报文（第一次握手），服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态
- 但服务端发送出去的 ACK + SYN 报文（第二次握手），无法得到未知 IP 主机的 ACK 应答（无法建立第三次握手），然后会重传，重传期间，半连接队列就会很满
- 久而久之就会占满服务端的半连接队列，使得服务器不能为正常用户服务

解决方式：
- 降低重传次数
- 开启 syncookies 功能：在不使用 SYN 半连接队列的情况下成功建立连接
  - 当 「 SYN 半连接队列」满之后，后续服务器收到 SYN 包，不会丢弃，而是根据算法，计算出一个 cookie 值
  - 将 cookie 值放到第二次握手报文的「序列号」里，然后服务端回第二次握手给客户端
  - 服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，将该连接对象放入到「 Accept 队列」


### 四次挥手

- 如果第一次挥手丢失：客户端（主动断开方）收不到ACK，会重传；如果一直收不到，超过了最大重传次数都没有收到ACK，则会直接断开连接
- 如果第二次挥手丢失：客户端会重传第一次挥手的信息
- 如果第三次挥手丢失：服务端重发断开连接的FIN信息
- 如果第四次挥手丢失：服务端重发断开连接的FIN信息
  - 当客户端收到服务端的第三次挥手的 FIN 报文后，就会回 ACK 报文，也就是第四次挥手，此时客户端连接进入 TIME_WAIT 状态
  - 在 Linux 系统，TIME_WAIT 状态会持续 2MSL 后才会进入关闭态

### 为什么 TIME_WAIT 等待的时间是 2MSL

MSL：报文最大生存时间

2MSL 的时间是从客户端接收到 FIN 后发送 ACK 开始计时的

如果被动关闭方没有收到断开连接的最后的 ACK 报文，就会触发超时重发 FIN 报文，另一方接收到 FIN 后，会重发 ACK 给被动关闭方， 一来一去正好 2 个 MSL

相当于：至少允许ACK报文丢失一次

### 为什么需要 TIME_WAIT 状态？

主动发起关闭连接的一方，才会有 TIME-WAIT 状态

- 防止历史连接中的数据，被后面相同四元组的连接错误的接收
  - 为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了 TIME_WAIT 状态
  - 状态会持续 2MSL 时长，这个时间足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的

- 保证「被动关闭连接」的一方，能被正确的关闭




### 重传机制

- 超时重传
  - 超时重传时间是以 RTO 表示（动态变化的）
  - 应该略大于报文往返 RTT 的值
  - 如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍

- 快速重传
  - 不以时间为驱动，而是以数据驱动重传
  - 当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段
  - 存在问题：假设发送方发了 6 个数据，编号的顺序是 Seq1 ~ Seq6 ，但是 Seq2、Seq3 都丢失了，那么接收方在收到 Seq4、Seq5、Seq6 时，都是回复 ACK2 给发送方，但是发送方并不清楚这连续的 ACK2 是接收方收到哪个报文而回复的， 那是选择重传 Seq2 一个报文，还是重传 Seq2 之后已发送的所有报文呢（Seq2、Seq3、 Seq4、Seq5、 Seq6） 呢？

- SACK：解决上述问题
  - 可以将已收到的数据的信息发送给「发送方」，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据

### 滑动窗口

- 有了窗口，就可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值

- 窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除

- 发送方的滑动窗口：接收到ACK后，滑动窗口移动，在窗口内，所有未发送的数据都可以发送，而不需要等待前面数据的确认号
- 接收端的滑动窗口：窗口内表示还没收到数据但是可以进行接收的，而当接收并发送ACK后，滑动窗口可以向后进行移动


### 窗口大小关系

并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。

因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系



### 糊涂窗口综合症

- 如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小
- 到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症

解决方法：
- 让接收方不通告小窗口给发送方：当「窗口大小」小于 min( MSS，缓存空间/2 ) ，也就是小于 MSS 与 1/2 缓存大小中的最小值时，就会向发送方通告窗口为 0，也就阻止了发送方再发数据过来
- 让发送方避免发送小数据


### 拥塞控制

- 拥塞窗口：cwnd
- 只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了拥塞
- 慢启动：指数增长，直到慢启动门限 ssthresh
- 拥塞避免：线性增长，当拥塞窗口 cwnd 「超过」慢启动门限 ssthresh 就会进入拥塞避免算法
- 拥塞发生：
  - 超时重传，ssthresh 设为 cwnd/2，cwnd设置为初始值
  - 快速重传：当收到三个一样的ACK，启动快速重传，ssthresh 设为 cwnd/2，cwnd = ssthresh + 3，表示网络可能出现了阻塞，所以需要减小 cwnd 以避免
- 快速恢复：拥塞窗口 cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了）

### TCP优化方法（？）

- 三次握手优化：
  - 三次握手建立连接的首要目的是「同步序列号」
  - 只有同步了序列号才有可靠传输，TCP 许多特性都依赖于序列号实现，比如流量控制、丢包重传等
  - 1、更改第一次握手的重传次数上限
  - 2、开启syncookies 功能，防止SYN攻击

### TCP延迟确认机制

- 当发送没有携带数据的 ACK，它的网络效率也是很低的，因为它也有 40 个字节的 IP 头 和 TCP 头，但却没有携带数据报文
- 为了解决 ACK 传输效率低问题，所以就衍生出了 TCP 延迟确认
  - 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方
  - 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送
  - 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK

### 已经建立连接，客户端挂了，服务器因为补发数据不知道，客户端再请求连接，会发生什么

- 如果客户端的 SYN 报文里的端口号与历史连接不相同
  - 此时服务端会认为是新的连接要建立，于是就会通过三次握手来建立新的连接
  - 原本的服务器连接：
    - 如果服务端发送了数据包给客户端，由于客户端的连接已经被关闭了，此时客户的内核就会回 RST 报文，服务端收到后就会释放连接
    - 如果服务端一直没有发送数据包给客户端，在超过一段时间后， TCP 保活机制（TCP keepalive）就会启动，检测到客户端没有存活后，接着服务端就会释放掉该连接
   
- 如果客户端的 SYN 报文里的端口号与历史连接相同
  - 处于 establish 状态的服务端如果收到了客户端的 SYN 报文（注意此时的 SYN 报文其实是乱序的，因为 SYN 报文的初始化序列号其实是一个随机数），会回复一个携带了正确序列号和确认号的 ACK 报文，这个 ACK 被称之为 Challenge ACK
  - 接着，客户端收到这个 Challenge ACK，发现序列号并不是自己期望收到的，于是就会回 RST 报文，服务端收到后，就会释放掉该连接


### 进程崩溃和主机掉线，分别会导致TCP连接的什么

- 主机掉线+没有开启保活机制+没有数据交互：如果客户端主机掉线，服务端的 TCP 连接将会一直处于 ESTABLISHED 连接状态，直到服务端重启进程
- 进程崩溃：
  - TCP 的连接信息是由内核维护的
  - 所以当服务端的进程崩溃后，内核需要回收该进程的所有 TCP 连接资源
  - 于是内核会发送第一次挥手 FIN 报文，后续的挥手过程也都是在内核完成，并不需要进程的参与，所以即使服务端的进程退出了，还是能与客户端完成 TCP四次挥手的过程
